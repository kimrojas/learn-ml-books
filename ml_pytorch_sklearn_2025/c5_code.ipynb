{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341e3d4d",
   "metadata": {},
   "source": [
    "# Compression Data via Dimensionality Reduction\n",
    "\n",
    "- An alternative to **Feature selection** is **Feature extraction**\n",
    "- Here we learn how to summarize content of a dataset by transforming into new feature subspace of lower dimensionality \n",
    "\n",
    "\n",
    "## Unsupervised dimensionality reduction via principal component analysis\n",
    "\n",
    "- feature extraction transforms or projects the data into a new feature subspace\n",
    "- feature extraction = data compression (in dimensionality reduction context) which\n",
    "  - improves storage space\n",
    "  - computational efficiency\n",
    "  - predictive performance (by reducing the curse of dimensionality) especially for non-regularized models\n",
    "\n",
    "### The main steps in pricipal component analysis\n",
    "\n",
    "- principal component analysis (PCA) - unsupervised linear transformation technique used as\n",
    "  - feature extraction\n",
    "  - exploratory data analysis\n",
    "  - denoising of signals in stock market trading\n",
    "  - analysis of genome data \n",
    "- PCA helps identify patterns based on correlation between features\n",
    "- PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal of fewer dimensions than the original one. \n",
    "- The orthogonal axes (pricipal component)\n",
    "  - directions of maximum variance\n",
    "  - given the constraint that the new feature axes are orthogonal to each other\n",
    "- PC1 and PC2 are the principal components\n",
    "- Using PCA for dimensionality reduction, \n",
    "  - construct a $ d \\times k$-dimensional transformation matrix $\\mathbf{W}$ \n",
    "  - maps the features of a training example $x$ onto a new $k$-dimensional feature subspace that is lesser than $d$\n",
    "\n",
    "$$\n",
    "\\textbf{x} = [x_1, x_2, \\dots x_d], \\mathbf{x} \\in \\Reals^d \\\\\n",
    "\\mathbf{W}  \\in \\Reals^{d \\times k} \\\\\n",
    "\\textbf{x}\\mathbf{W} = \\mathbf{z} \\\\\n",
    "\\mathbf{z} = [z_1, z_2, \\dots z_k], \\mathbf{z} \\in \\Reals^k \\\\\n",
    "$$\n",
    "\n",
    "- $k << d$\n",
    "- the first principal component will have the largest variance\n",
    "- succeeding pricipal component will have the largest variance given that they are uncorrelated (orthogonal) to  other  pricipal components\n",
    "- if PCA directions are  highly sensitive to data scaling, we need to standardize the features prior to PCA. We want to assign equal importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7da77",
   "metadata": {},
   "source": [
    "**PCA algorithm**\n",
    "\n",
    "1. standardize the $d$-dimensional dataset\n",
    "2. Contruct a covariance matrix\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors\n",
    "5. select $k$ eigenvectors corresponding to the $k$ largest eigenvalues ($k$ is the dimensionality of the new feature subspace)\n",
    "6. Construct a projection matrix $\\mathbf{W}$ from the \"top\" $k$ eigenvectors\n",
    "7. Transform the $d$-dimensional input dataset $\\mathbf{X}$ using the projection matrix  $\\mathbf{W}$ to obtain the new $k$-dimensional feature subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0afb7",
   "metadata": {},
   "source": [
    "### Extracting the principal component step by step\n",
    "\n",
    "1. Standardizing the data\n",
    "2. Constructing the covariance matrix\n",
    "3. Obtaining the eigenvalues and eigenvectors of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b2a82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
